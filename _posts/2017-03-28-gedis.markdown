---
layout: post
---
Gedis

前言
萌生做一个支持集群的Redis客户端来自一个非常现实的问题。由于阿里云的网络拓扑结构，经常会发生数据丢失，或是Redis抛出一些莫名其妙的网络错误。单点Redis这个情况就无法满足我们高并发下的业务需求了。

为什么不用Codis
因为服务器遇到了这个问题，所以就很自然的萌生了上一套Redis集群的想法。在当时最新Redis版本才2.8.3的时代里，官方的集群功能是刚刚发布了正式版，可用性还待检测，开源界比较出名的有Twitter的Twemproxy，还有一个豌豆荚的Codis。本着支持国产的想法，我们就试着研究和部署了一下Codis的集群。Codis的集群结构属于比较工程化的，它基本没有对Redis服务器的代码做大的改动改动，也没有基于服务器做数据一致性的只是新添加了一些对Zookeeper的支持和一些集群化的功能，然后他所有的操作另外起了一层中间件redis-proxy，在中间件中完成诸如分发，选举，HA等的功能，包括兼容原生Redis客户端的一些命令支持。
关于Codis的具体设计就不再赘述了，但是我仍然要说Codis是一个非常不错的Redis集群系统，整个设计也是非常标准，非常可视化的，更何况还支持一个dashboard，所有的东西都一目了然，不同的功能对应到不同的模块，文档也很齐全。所以我们在最开始的时候，确实是选择了使用Codis来做集群。但是一个罕见却严重的BUG最终使我们放弃了它。
在部署Codis集群之前，我们在本地和线上的测试集群都进行了大量的测试，最终确定了他的高可用性。然后我们在某一天的凌晨就更新了我们的服务器，开始使用Codis做我们的分布式Session，观察了一两个小时一切正常之后，我们以为大功告成，就去休息了。
但是到了第二天下午，客服突然告诉我们，有很多客户反应验证码能刷新但是无法通过。
我们当时赶紧检查了服务器的日志，发现并没有报错，然后我们以为Redis挂了，检查了Redis的进程，存在没问题。我们当时都非常纳闷，为什么进程存在，却读不到任何数据。最后试着看了一下Redis使用，发现占用内存941K。
我们赶紧回滚了代码，重新搭回了旧的Redis。然后开始在Codis寻找到底是什么原因导致了这个问题。因为有守护进程的存在，我们没法去追踪是否有Redis服务器Down机了，所以就只能尝试看系统日志，发现日志中最后一条是，主节点Down机，开始进行选举，然后就没有了。令人奇怪的人，这个部分我们在之前进行过大量的测试，也正是因为Codis宣称的高可用性，我们试过主节点Down机，副节点Down，主副一起Down机，几台副节点一起Down机，都没有发现有这个问题。
寻找这个BUG的曲折离奇就不再赘述了，总之最后的结果是Codis的选举机制存在十多秒的延迟，如果在主节点Down机后，在某一个特定的时间段，也就是选举开始到选举成功时，被选为新的主节点的副节点也Down机的话，就会造成这种情况，出现这个问题的直接原因是在选举开始时会读取一次Zookeeper中的节点，然后从节点中选Master，成功之后节点中的某一个就变成Master，但是这时候被选为Master的已经Down机了，自然就无法收到自己被选为Master的数据包了。但是Codis本身做了二次确认，心跳包是能追踪到这个问题的，但是，Codis没有考虑到守护进程会自动让这个Down机的Redis重启，再次发心跳包时，检测到的这个新主节点却是正常的，所以最后的结果就是，Codis认为已经选举成功，但是被选为Master的因为Down机却未收到这个请求，最终导致了整个系统出现了未捕捉到的异常。不过根本原因的话，一还是Codis的心跳机制不完善，没有考虑到这种Master已经选举出来但是却没有成功当选的特殊情况，另一方面，也是对Zookeeper的依赖太强。因为是非正常结束进程，Zookeeper的心跳机制有十数秒的延迟，最后没有做出反应也是非常正常的。
这个BUG可能没什么，但最终让我们决定放弃Codis的原因还是因为豌豆荚工程师对我们汇报这个BUG时的掩饰，在我们都已经完美复现了多次这种情况，并且在Issue中，也有其他人说到了这个问题的情况下，他们就是不承认有这个问题。大半年后，在某次“小更新”中，Codis宣称自己解决了一个可能会影响使用的BUG。当然，这都是后话了。
在这里我没有要贬低Codis的意思，事实上Codis是一个很不错的集群系统，时间也证明了确实它却能能够提供一整套的高可用的分布式缓存解决方案。
